---
phase: 01-web-infrastructure-and-core-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/gridworld/server.py
  - src/gridworld/agent.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "FastAPI server responds to HTTP requests on localhost:8000"
    - "WebSocket endpoint accepts connections and echoes messages"
    - "Q-learning agent selects actions using epsilon-greedy policy"
    - "Q-learning agent updates Q-table based on experience tuples"
    - "Agent epsilon decays over training episodes"
  artifacts:
    - path: "src/gridworld/server.py"
      provides: "FastAPI app with WebSocket endpoint and static file serving"
      min_lines: 50
    - path: "src/gridworld/agent.py"
      provides: "Q-learning agent with epsilon-greedy and Q-table updates"
      min_lines: 80
    - path: "requirements.txt"
      provides: "FastAPI and Uvicorn dependencies"
      contains: "fastapi"
  key_links:
    - from: "src/gridworld/server.py"
      to: "starlette.websockets.WebSocket"
      via: "import and websocket_endpoint decorator"
      pattern: "@app\\.websocket"
    - from: "src/gridworld/agent.py"
      to: "numpy array for Q-table"
      via: "Q-table initialization and updates"
      pattern: "np\\.zeros.*q_table|self\\.q_table"
---

<objective>
Establish backend foundation with FastAPI server infrastructure and complete Q-learning agent implementation.

Purpose: Create the server and algorithm foundations that training loop will build upon. These components are independent and can be developed in parallel with frontend work.

Output: Working FastAPI server with WebSocket capability + fully functional Q-learning agent ready for training loop integration.
</objective>

<execution_context>
@/Users/luckleineschaars/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luckleineschaars/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-web-infrastructure-and-core-loop/01-CONTEXT.md
@.planning/phases/01-web-infrastructure-and-core-loop/01-RESEARCH.md
@src/gridworld/config.py
@src/gridworld/environment.py
</context>

<tasks>

<task type="auto">
  <name>Create FastAPI server with WebSocket endpoint</name>
  <files>src/gridworld/server.py, requirements.txt</files>
  <action>
    Create FastAPI application in src/gridworld/server.py following the ConnectionManager pattern from RESEARCH.md.

    Implementation specifics:
    - Create ConnectionManager class with connect(), disconnect(), and broadcast() methods
    - Create FastAPI app instance
    - Add WebSocket endpoint at /ws with connection lifecycle handling (connect, receive loop, disconnect)
    - Mount StaticFiles at root to serve static/ directory (will be created in Plan 02)
    - Add basic message handler skeleton that logs received messages (full implementation in Plan 03)
    - Include proper error handling for WebSocketDisconnect

    Add to requirements.txt:
    - fastapi>=0.104.1
    - uvicorn[standard]>=0.24.0
    - starlette>=0.27.0

    Reference RESEARCH.md Pattern 1 (FastAPI WebSocket with ConnectionManager) for exact code structure. Use async/await throughout - this is an async-first application.
  </action>
  <verify>
    1. Run: pip install -r requirements.txt
    2. Start server: python -m uvicorn src.gridworld.server:app --reload --host 127.0.0.1 --port 8000
    3. Server starts without errors and logs "Application startup complete"
    4. Test WebSocket with Python client script or wscat if available
  </verify>
  <done>
    - FastAPI server runs on localhost:8000
    - WebSocket endpoint at ws://localhost:8000/ws accepts connections
    - ConnectionManager successfully manages connection lifecycle
    - Requirements.txt includes FastAPI, Uvicorn, Starlette
  </done>
</task>

<task type="auto">
  <name>Implement Q-learning agent with epsilon-greedy and Q-table updates</name>
  <files>src/gridworld/agent.py</files>
  <action>
    Complete the Q-learning agent implementation in src/gridworld/agent.py. The file currently exists but is stubbed (only 1 line).

    Implementation specifics:
    - Create QLearningAgent class that accepts QLearningConfig (from config.py) and grid_size
    - Initialize Q-table as numpy array with shape (grid_size, grid_size, 4) for 4 actions (up, down, left, right)
    - Initialize Q-values to zeros
    - Store epsilon, learning_rate, discount_factor from config
    - Implement epsilon-greedy action selection:
      * With probability epsilon: select random action
      * Otherwise: select action with highest Q-value for current state
    - Implement Q-table update using standard Q-learning formula:
      * Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]
    - Implement decay_epsilon() method that multiplies epsilon by epsilon_decay (from config)
    - Add get_q_values(state) helper to retrieve Q-values for a state (used later for visualization)

    Action encoding: 0=up, 1=down, 2=left, 3=right (match environment conventions if they exist, or establish this standard).

    Use numpy for Q-table operations for efficiency. The agent should be stateless except for Q-table and epsilon - all other params come from config at init.
  </action>
  <verify>
    Write simple test in Python REPL:
    ```python
    from src.gridworld.agent import QLearningAgent
    from src.gridworld.config import QLearningConfig

    config = QLearningConfig(learning_rate=0.1, epsilon_start=1.0)
    agent = QLearningAgent(config, grid_size=5)

    # Test action selection
    state = (0, 0)
    action = agent.select_action(state)
    assert action in [0, 1, 2, 3], "Action must be 0-3"

    # Test Q-update
    agent.update(state, action, reward=1.0, next_state=(0, 1), done=False)
    assert agent.q_table[0, 0, action] != 0, "Q-value should update"

    # Test epsilon decay
    initial_epsilon = agent.epsilon
    agent.decay_epsilon()
    assert agent.epsilon < initial_epsilon, "Epsilon should decay"
    ```
  </verify>
  <done>
    - QLearningAgent class exists with __init__, select_action, update, decay_epsilon methods
    - Q-table initialized as 3D numpy array (grid_size x grid_size x 4)
    - Epsilon-greedy policy implemented correctly (random with prob epsilon, greedy otherwise)
    - Q-value updates follow standard Q-learning formula
    - Agent integrates with existing QLearningConfig from config.py
  </done>
</task>

</tasks>

<verification>
**Backend infrastructure verification:**
1. FastAPI server starts successfully: `uvicorn src.gridworld.server:app --reload`
2. WebSocket connection accepts at ws://localhost:8000/ws
3. ConnectionManager handles connect/disconnect lifecycle without errors

**Agent implementation verification:**
1. Q-table initializes with correct shape and zero values
2. Epsilon-greedy selection produces valid actions (0-3)
3. Q-values update after experience tuples
4. Epsilon decays correctly over time
5. Agent can be instantiated with GridWorldConfig parameters
</verification>

<success_criteria>
Plan succeeds when:
1. FastAPI server runs and accepts WebSocket connections
2. Q-learning agent implements epsilon-greedy action selection
3. Q-learning agent updates Q-table using standard Q-learning formula
4. Both components are independently testable
5. Code follows async patterns (server) and numpy best practices (agent)
6. No external dependencies beyond FastAPI, Uvicorn, and existing numpy/gymnasium
</success_criteria>

<output>
After completion, create `.planning/phases/01-web-infrastructure-and-core-loop/01-01-SUMMARY.md`
</output>
