# Learning Notes

This directory contains notes, insights, and lessons learned while implementing RL algorithms.

## Purpose

Document your understanding as you build. These notes will help you:
- Solidify concepts by explaining them in your own words
- Debug issues by understanding what _should_ happen
- Review and remember key insights from previous sessions

## Suggested Topics

- **Q-Learning fundamentals**: Bellman equation, value iteration, policy extraction
- **Exploration strategies**: epsilon-greedy, UCB, Boltzmann
- **Convergence**: When does Q-learning converge? What can go wrong?
- **Reward shaping**: Designing rewards that encourage learning
- **State representation**: How to represent complex states (important for Solitaire)
- **Deep RL**: Why neural networks? When are they needed?
- **Common pitfalls**: Things that went wrong and how you fixed them

## Format

Feel free to organize however works for you. Some ideas:
- One note per concept
- Notes organized by phase (gridworld, solitaire)
- Q&A format
- Problem-solution journal
